---
title: "Literature Review"

documentclass: "elsarticle"

Author1: "Fiona Ganie"  
Ref1: "University of Cape Town" 
Email1: "GNXFIO001\\@myuct.ac.za" 

BottomRFooter: "\\footnotesize Page \\thepage\\" 
addtoprule: TRUE
addfootrule: TRUE 
bibliography: Tex/ref.bib  
RemovePreprintSubmittedTo: TRUE

toc: no                        
numbersections: yes             

fontsize: 12pt                  
linestretch: 1.5                
link-citations: TRUE          

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 
    fig_height: 3.5
    include:
      in_header: Tex/packages.txt 
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')

```

# Introduction \label{Introduction}

The ability to forecast exchange rates is of great importance to policy makers, businesses and speculators. Policy makers are interested in forecasting exchange rates to assess how its movement will impact inflation, and how monetary policy will need to be adjusted in the future to keep inflation within the inflation target [@fahimifard2009]. Businesses need to assess how future movements in exchange rates will affect the cost of importing goods and the demand for goods exported, while speculators aim to identify opportunities in price movements which can be exploited to make a profit. 

Producing accurate forecasts of exchange rates has proven to be a difficult task due to the noise of the time series [@abu1996]. Some economists argue that the exchange rate market is very efficient and future movements in price are unpredictable [@abu1996]. This belief in the efficient market hypothesis has led many researchers to model exchange rates using a Random Walk (RW) model [@abu1996]. Although the RW model has produced superior forecasts, some researchers believe that future prices depend on present and past events, and other techniques such as the autoregressive integrated moving average (ARIMA) model has been applied to forecast exchange rates [@abu1996].

The ARIMA model is one of the most commonly used methods in financial forecasting [@fahimifard2009]. It is known for its attractive quality of being a rich class of processes, making it possible to find a model that adequately fits the data [@fahimifard2009]. Some research has shown that the nature of exchange rate data is non-linear, and this non-linearity may not be captured adequately by the ARIMA model [@zhang1998]. Linear parametric forecasting techniques such as the generalised autoregressive conditional heteroskedasticity (GARCH) model has been proposed as a solution to this problem [@zhang1998]. Although the GARCH model takes care of the non-linearity in the data, the pre-specification of the model may prevent it from capturing all the different non-linearities which may be present in the data. 

With the evolution of computational power, other non-linear non-parametric techniques such as Artificial Neural Networks (ANN), Support Vector Regression (SVR), Fuzzy Logistic Systems and hybrid versions of the ARIMA model have been proposed. The ARIMA model has been used as the standard benchmark model against which these other methods have been compared. Although they have produced significantly better results than the ARIMA model, building these models is a complex task. The ARIMA model is more tractable, less computationally expensive, and has been used as the building blocks for more advanced models such as the autoregressive fractionally integrated (ARFIMA), Fuzzy ARIMA, and seasonal ARIMA (SARIMA) model which have been used for exchange rate forecasting.

Facebook recently open sourced their forecasting model Prophet. Prophet is said to be preferable over the ARIMA model as it has the ability to forecast at scale, is easily interpretable and has parameters that can be intuitively adjusted [@taylor2017]. This paper aims to compare the forecasts produced by the traditional ARIMA model and Prophet through an evaluation of Bitcoin/ZAR, using a Mincer-Zarnowitz approach of measuring forecast accuracy. Section 2 will consider the origins of the ARIMA model and how it has performed and evolved over time by exploring empirical studies where it has been used for forecasting exchange rates. Section 3 will also consider other techniques that have been used to forecast exchange rates and how they have been compared to the ARIMA model. Section 4 will look at hybrid models that were inspired by the ARIMA model, while section 5 looks at forecasting with Prophet. Finally, section 6 will look at the Mincer-Zarnowitz approach to evaluating forecasts. 


# Forecasting Exchange Rates with the ARIMA Model

## Birth of ARIMA
ARIMA models became highly popular since its introduction by Box and Jenkins, when it was shown that they could outperform complex econometric models in a variety of situations [@spyros1997]. The ARIMA model expresses the process {$y_t$} as a function of the weighted average of past values of the process and lagged values of the residuals. The weighted average of the past p values of the process represents an autoregressive (AR) process of order p. It feeds back past values of the process into the current value, inducing correlation between all lags of the process. The weighted average of the q lagged residuals represents a moving average (MA) process of order q. The purpose of mixing the MA process with the AR process is to reduce the large number of past values required by AR processes and to control for the autocorrelation which it creates between lagged values of the process. The combination of the AR(p) and MA(q) process results in a more parsimonious model, and forms a stationary autoregressive moving average (ARMA(p,q)) process defined as:

$$ 
	y_t = c + \sum_{i = 1}^{p} \theta_{i} y_{t-i} + \sum_{i = 1}^{q} \phi_{i} e_{t-i}+ e_t \label{eq1} \\ \notag
$$
where c is a constant and {$e_t$} is a white noise process with zero mean and variance $\sigma^2$.

The ARIMA(p,d,q) model generalises the ARMA model in that it includes both stationary and non-stationary processes. The parameter d is the degree of differencing required to render the process stationary. If d is equal to zero the process is stationary and equivalent to an ARMA model, and if d is strictly positive the process requires differencing to make it stationary. The ARIMA model can be defined succinctly using the backward shift operator B, which shifts the process back by one unit of time, and is defined as $By_t = y_{t-1}$. The ARIMA model has the form:
 
$$ 
	(1-\sum_{i = 1}^{p} \theta_{i} B^i)(1-B)^dy_t = c + (1 + \sum_{i = 1}^{q} \phi_{i} B^i)e_t \label{eq2} \\ \notag 
$$
where c is a constant and {$e_t$} is a white noise process with zero mean and variance $\sigma^2$.

## The Box-Jenkins Methodology and Automatic Selection
Box and Jenkins proposed a set of guidelines that can be followed when selecting ARIMA models. This systematic procedure to designing ARIMA models has made them highly popular [@spyros1997]. It consists of a four-stage iterative process in which: 1) the process is either transformed or differenced to detrend and stabilise the variance of the data, 2) the autocorrelation and partial autocorrelation plots are used to determine the order of p and q, 3) the parameters of the model are estimated, 4) a diagnostic check is performed to ensure that the residuals are a white noise process. If the residuals are not white noise steps 2-4 are repeated until a satisfactory model is identified. On the contrary, if the diagnostic check shows that the residuals are random then the developed model is the final model used for forecasting.

Due to the large number of forecasts made, it is useful to have an automatic procedure which is able to select the appropriate ARIMA model to fit the data. The auto.arima function in R is able to automatically choose the order of the parameters p, q and d. The order of differencing is determined by using the KPSS test [@ruppert2015]. The KPSS test checks the null hypothesis of stationarity and sets d to zero if the null hypothesis is accepted, otherwise it iteratively increases d by 1 and tests the null hypothesis until it is accepted [@ruppert2015]. Once the order of differencing has been determined, the order of p and q are chosen based on Akaike’s Information Criterion (AIC) or the Bayesian Information Criterion (BIC) [@ruppert2015].


## Empirical Applications
ARIMA models have been commonly used in financial forecasting and are popular for observing stock prices and exchange rates due to its power and statistical properties [@lin2012]. They have been used as a benchmark to compare new forecasting techniques which have emerged over time. @kamruzzaman2003 applied an ARIMA model to forecast the exchange rate between the Australian dollar and six other currencies. Its results were used to evaluate the performance of three Neural Network models. The Normalised Mean Square Error (NMSE) and Mean Absolute Error (MAE) were used to measure the forecast errors while the Directional Symmetry (DS), Correct Up trend (CU) and Correct Down trend (CD) were used to measure the accuracy of the direction of the forecasts. Similarly, @lin2012 employed an ARIMA model to forecast the Taiwan New Dollar exchange rate. The aim of applying the ARIMA model was to analyse the ability of a proposed hybrid model created from the Squares Support Vector Regression (LSSVR) and Empirical Mode Decomposition (EMD) model. @lin2012 used the same indicators as @kamruzzaman2003 to evaluate the accuracy of the direction of the forecasts. To measure the forecast errors produced they applied the mean absolute percentage error (MAPE), root‐mean‐square error (RMSE) and mean absolute difference (MAD). @khashei2012 also investigated the ability to forecast exchange rates by considering the British pound and US dollar, over a short and long forecast horizon. They compared the predictive capability of their proposed hybrid model consisting of an ARIMA and a Probabilistic Neural Network (PNN), to the traditional ARIMA model, and justified the use of the ARIMA as a benchmark, by claiming that it is the most important linear model. The ARIMA model has also yielded satisfactory results when predicting exchange rates. @nwankwo2014 forecasted the Nairo/dollar rate and diagnostic testing revealed that the ARIMA(1,0,0) model was the best fit for the data based on the AIC.    

The ARIMA model is attractive as it is tractable and produces good forecasts in short time periods where more than 100 observations are used [@tseng2001]. 
Although the ARIMA model has the advantage of ease of implementation and flexibility, it fails to capture the non-linearity and volatility present in exchange rate data.

## Evolution of ARIMA
Over time, the ARIMA model has evolved to cater for a wider variety of data that it can be applied to, and to compensate for some of its shortcomings. The most popular versions of the ARIMA model that has been implemented in exchange rate forecasting is the Seasonal ARIMA (SARIMA) and Fractional ARIMA model (FARIMA). 

The SARIMA model was introduced by Box and Jenkins as a generalisation of the ARIMA model to capture the periodic behaviour of data. SARIMA models extend the ARIMA class by including a term for seasonal differencing. Exchange rate data that has been observed have shown seasonality, and have been attempted to be fitted by SARIMA models [@etuk2013]. @etuk2013 modelled the Naira/CFA Franc exchange rate which exhibited monthly seasonality using an additive SARIMA model, to demonstrate that it can be a useful fit to the data. Their results showed that the SARIMA model adequately described the variation in the exchange rate series.  This agrees with the results of a study by @etuk2012 which fitted a multiplicative SARIMA model to the Naira/Dollar exchange rates.

The ARFIMA model is another generalisation of the ARIMA model in that the degree required to make the data stationary can assume any real value, and is no longer restricted to the integer domain. While the ARIMA model displays exponentially decaying autocorrelation between observations the further apart they are in time, the ARFIMA model has the ability to capture the dependence between observations that are widely spread apart in time [@cheung1993].  This makes the ARFIMA model more parsimonious since it can capture long memory in data as well as short term dynamics [@cheung1993]. @cheung1993 fitted the ARFIMA model to examine five exchange rates, and found that there was strong evidence of long memory in the exchange rate time series. Although the ARFIMA model fitted the data well, it failed to beat the random walk in out-of sample forecasts.  

GARCH models were later developed due to the failure of ARIMA models to capture the volatility in financial markets [@anastasakis2009]. They have been widely used to forecast the volatility of stock returns, interest rates and exchange rates and are useful for portfolio allocation decisions as well as option pricing [@cermak2017]. @cermak2017 employed a GARCH(1,1) model to analyse the volatility of Bitcoin in countries in which Bitcoin is commonly traded, while Hsieh (1989) applied a GARCH model to investigate five exchange rates. Hsieh’s (1989) results showed that although the GARCH model outperformed the random walk, some non-linear information still remained in the residuals.  In a similar study, @fahimifard2009 illustrated that the GARCH model outperformed the ARIMA, while Neural Networks outperformed both the GARCH and ARIMA model.

# Other techniques used to forecast exchange rates
Over time, financial forecasting methods have moved away from linear models like ARIMA and GARCH, to soft computing techniques. These more complex techniques are non-linear and can fit complex time series more easily [@castillo2002]. Unlike regression analysis and the ARIMA model, soft computing techniques do not impose structural assumptions on the model apriori [@castillo2002]. Some of the most commonly used artificial intelligence methods used to forecast exchange rate data are Neural Networks and the Fuzzy Logistic Systems.

## Neural Networks
Artificial Neural Networks are commonly used in financial forecasting and are more advantageous than other non-linear forecasting methods. They can approximate any continuous function, are data driven, and can adapt to non-stationary environments [@khashei2011]. Neural Networks mimic the structure of the brain and consists of three layers: the input, hidden and output layer, each of which contain nodes which are representative of the neurons in our brain. The job of the nodes in the input layer is to send signals along connections to the nodes in the hidden layer, where the connections are representative of the synapses in our brain. The nodes in the hidden layer uses an activation function and assigns a weight to each connection according to the level of importance of information. The value in the hidden layer is then sent along connections to the output layer, where weights are again assigned to each connection. The error is then computed in the output layer and is backpropagated through the network. The weights are adjusted according to the error that they contribute, allowing the ANN to be trained and learn. This learning process is repeated until the network achieves the desired output. 

ANN’s have had many successful applications in forecasting exchange rates. In a study done by @fahimifard2009, the ANN was found as an effective way to improve the forecasts of exchange rates. Superior results were produced when compared to the ARIMA and GARCH model using the RMSE, MSE and MAD as measure of performance.  Similar results were found by @zhang1998 however the ANN failed to outperform the random walk over long forecast horizons. Contrary to the findings above, @kuan1995 attempted to investigate how well feedforward and recurrent networks capture the non-linearity of exchange rate data and obtained conflicting results with neural networks having significantly better market timing ability or lower out-of-sample Mean Square Prediction Error (MSPE) compared to the ARMA models for some exchange rates only. @franses1998 suggested that the non-linear feature of exchange rates that are picked up by ANNs may actually be due to neglected GARCH effects. They investigated the application of ANNs to forecast exchange rates and found that the presence of GARCH in the data may mistakenly lead one to believe that the returns can be forecasted on the exchange rates themselves. Their results show that there is no gain in producing out-of-sample forecasts using ANN if the data is not truly non-linear.

Although neural networks have been broadly applied in financial forecasting, the process of building neural networks is a complex task and there is no consistent method to design them compared to the traditional Box-Jenkins ARIMA model. Unlike ARIMA models, the performance of ANNs is sensitive to many modelling factors such as the number of input nodes and the training sample size [@zhang1998].  

## Fuzzy Logistic Systems
Fuzzy Logistic Systems were initially developed to solve problems involving linguistic terms, and have been used successful in financial forecasting [@khashei2009]. Unlike computers, the human decision-making process includes a range of possibilities spanning ‘yes’ and ‘no’. Fuzzy logic tries to imitate this reasoning and decision-making process by taking a discrete decision space that consists of a large number of states and turning it into a continuous space. This allows for more finer decisions to be provided as opposed to discrete decisions.  

@santos2007 investigated how well fuzzy systems and neural networks perform compared to the traditional ARMA and GARCH model. He examined the forecasts of  Brazilian exchange rate returns looking at different frequencies of the series and compared their one-step-ahead forecasts. By analysing the RMSE, U-Theil inequality index, percentage of corrected predicted signals (CPS), and the Pesaran-Timmermann (PT) predictive failure statistic, he found that the Fuzzy Systems and Neural Networks achieved higher returns based on the forecasts they produced. Similar results were found by @khashei2009 when he analysed the predictive capabilities of Fuzzy Systems, Neural Networks, the traditional ARIMA model, and a Fuzzy ARIMA model. The MAE and MSE of the ARIMA model were higher compared to the other models.

Although Fuzzy Logistic Systems has the advantage over ARIMA models that they can be applied to data with few observations available, it gives acceptable rather than accurate reasoning and are more suitable for problems which do not require high accuracy. Like ANN’s there is no systematic approach to designing Fuzzy Logistic Systems and they are only understandable when simple. 

# Hybrid ARIMA Models
Over time, many researchers began to think of ways in which to harness the advantages of tractable linear models such as the ARIMA, and of more complex non-linear models. By combining the ARIMA model with other forecasting methods, the advantages of both forecasting methods are leveraged while simultaneously improving their limitations. Some of the hybrid models which have been commonly used are the Fuzzy ARIMA and ANN-ARIMA model. 

##Fuzzy ARIMA
The ARIMA model produces very accurate forecasts over short time horizons however it has the limitation of requiring more than 100 observations of historical data [@tseng2001]. In a world that is constantly changing and with the rapid advancement of technology, access to large amounts of historical data is difficult to obtain. On the contrary, a fuzzy regression model requires little historical data however produces wide prediction intervals if extreme values are present in the data. The Fuzzy ARIMA model combines the ARIMA and fuzzy regression model to exploit the advantages of both models while simultaneously overcoming their limitations. @tseng2001 proposed applying a Fuzzy ARIMA model to forecast the exchange rate of Taiwan dollars to US dollars to demonstrate the model’s appropriateness and power. The Fuzzy ARIMA not only produced forecasts that were superior to the ARIMA and fuzzy time series models but also provided an upper and lower bound which can be used by decision makers to determine the best and worst possible situations.  

##ANN-ARIMA
Although ARIMA models are powerful, they require non-stationary data to be differenced and impose prior assumptions onto the distribution of the data [@ince2006]. In contrast, machine learning techniques such as ANNs do not impose any assumptions onto the data generating process however, being data-driven, are sensitive to the number of input nodes used. The ANN-ARIMA model draws on the strengths of both models and overcomes these individual difficulties. @ince2006 created an ANN-ARIMA model by using the ARIMA to determine the number input nodes required by an ANN for three exchange rates. When the ANN-ARIMA model was compared to the pure ARIMA, the hybrid model outperformed the ARIMA based on the MSE.  The findings of @khashei2011 study agree with these results. They implemented an ARIMA model and used its residuals together with past observations of the data as inputs for the ANN. Their hybrid model had superior in-sample and out-sample forecasts compared to the random walk, linear AR and ANN model.

Although the traditional ARIMA models produce less superior forecasts than its hybrid forms and other complex non-linear techniques, its forecasts are still satisfactory. They are simple models that are easy to implement and have a consistent method of model design and selection. ARIMA models are also more robust and efficient than complex structural models in relation to short-run forecasting. The fact that they have been used as the foundation for more advanced models and have commonly been used as a benchmark for comparison justifies it as a good starting point to compare it to Facebook’s forecasting method, Prophet, that was recently released.  

# Forecasting with Prophet 
The techniques that have been considered for exchange rate forecasting thus far, require the analyst to have vocational knowledge about time series. Prophet differs to traditional time series models in that it is flexible and can be customised by a large number of non-experts who have little knowledge about time series, however have domain knowledge about the data generating process. Prophet allows for a large number of forecasts to be produced across a variety of problems and consists of a robust evaluation system that allows for a large number of forecasts be evaluated and compared. This is Facebook’s definition of forecasting at scale.

## Construction of Prophet
Prophet consists of a decomposable model of the form:

$$ 
	y(t) = g(t) + s(t) + h(t) + e_t
$$
where the components of the model represent the growth, seasonality and holiday respectively, and $e_t$ is white noise.

These components consist of linear and non-linear functions of time. This differs to ARIMA models in which future values of the process are linear functions of previous observations and lagged residuals. Prophet is more like a Generalized Additive Model (GAM) which is a regression model that consists of non-linear and linear regression functions applied to predictor variables [@taylor2017]. Prophet, like the GAM, frames the forecasting problem as a curve fitting exercise, using backfitting to find the regression functions. The GAM is fitted quickly, allowing the analyst to interactively change the model parameters [@taylor2017].

The growth component is modelled in a similar way to population growths which use a logistic growth model [@taylor2017]. Populations typically grow non-linearly (although the growth component could also be linear) up to an upper bound known as the carrying capacity, and remains constant thereafter. The rate at which the population grows changes over time, and this is accounted for by including changepoints in the model where the growth rate can be automatically selected and may be adjusted [@taylor2017]. This allows non-experts with knowledge about events that may affect growth to use the parameter as a knob and adjust it to increase or decrease the number of changepoints [@taylor2017]. It also allows for the analyst to add changepoints which the automatic selection procedure may have missed [@taylor2017]. Furthermore, analysts may also specify the carrying capacity and adjust it based on their knowledge of the total market size [@taylor2017]. 

The decomposable form of the model allows for components to be easily added to it. This allows for multiple seasonality components with different periods to be added to the model. The variance of the parameters of the seasonality component’s distribution can be adjusted by analysts to smooth the model and change how much of historical seasonality is projected to the future [@taylor2017]. 

The name, date, and country of past and future holidays and events may also be inputted by the analyst into a list [@taylor2017]. By specifying the country in which the events take place or the holidays occur, separate lists can be populated for global events/holidays and country-specific events/holidays. The union of the two lists are then used for forecasting. Like seasonality, the variance of the parameters of the holiday component’s distribution, can be adjusted by analysts to smooth the model [@taylor2017].

## Empirical Applications
Prophet’s Bayesian approach to forecasting allows the analyst to incorporate their expert knowledge in the model building process and has produced significantly improved forecasts compared to the ARIMA model. @taylor2017 forecasted the number of events on Facebook using Prophet. The time series was impacted by holidays, had strong multi-period seasonality, and a piecewise trend. The forecasts produced by Prophet were compared to common automatic forecasting techniques such as exponential smoothing, ARIMA, and the seasonal naïve model, as well as to simple models such as the naïve model. While exponential smoothing and the seasonal naïve model were quite robust, the ARIMA forecasts were fragile. No model besides Prophet accounted for the dips around holidays and the upward trend of the time series towards later observations. 

## Semi-Automatic Selection
When a large number of forecasts are produced, manually identifying problematic forecasts becomes a time consuming and difficult task. Prophet provides a semi-automated forecast evaluation system that selects the best model that fits the data. When there are large forecast errors, the forecasts are flagged so that the analyst can explore the cause of the errors, identify and remove potential outliers and either adjust the model or choose a more appropriate model [@taylor2017]. Unlike ARIMA’s fully-automated evaluation system, Prophet provides interactive feedback and keeps the analyst in the loop. 

Prophet’s ability to forecast at scale enables it to model a wide variety of data and may be able to capture the non-linearity present in exchange rates. In contrast, the ARIMA model fails to capture the non-linearity inherent in exchange rate data and only produces good forecasts over short time horizons. Prophet is also a flexible model that has intuitive parameters which can be easily interpreted and modified by human beings [@taylor2017]. This may be useful to non-experts who have domain knowledge about factors that may affect the movement of exchange rate prices. Furthermore, unlike the ARIMA model, Prophet can produce forecasts over irregular time intervals and allows for missing values in the time series without the need for interpolation [@taylor2017]. If Prophet produces forecasts that are as good as the ARIMA model when forecasting Bitcoin/ZAR, it can be compared to more complex forecasting methods that are less tractable such as the hybrid models and machine learning techniques seen earlier. 

# Mincer-Zarnowitz Approach to Forecast Evaluation 
The Mincer-Zarnowitz approach to evaluating forecast accuracy is commonly used and can be useful when comparing the forecasts produced by Prophet and the ARIMA model. @mincer1969 proposed an absolute and relative measure to evaluate forecast accuracy. Absolute measures consider the distance between actual and predicted values. To analyse the absolute errors produced by the forecasts, the observed values are regressed against the predicted values. The intercept of the regression equation represents the mean distance between the observed and predicted values while the slope represents the correlation between the residual errors and the predicted values. A zero intercept implies that the forecasts are unbiased and do not overestimate or underestimate the data, while a unity slope implies that the forecasts are efficient and uncorrelated with the residual errors. A joint hypothesis test is performed to check this efficiency and bias of the forecasts, and models which produce the best results are selected.

Absolute forecast measures cannot be used to make comparisons between forecasts with different scales or economic variables. Furthermore, the size of forecasting errors is not as significant as the consequences of forecasting errors and how they impact the decision making process. Hence, rather than using absolute measures of forecast accuracy, @mincer1969 suggested the use of relative measures. Relative accuracy analysis allows for meaningful comparisons of different forecasting methods to be made. It uses an index which considers the ratio of the MSE of the forecast to the MSE of a benchmark forecasting method. A useful benchmark that may be used is an extrapolation of the data history, as it is a cost effective and accessible method, however any method which is relevant for comparison may be used. This ratio is known as the Relative Mean Square Error (RM) for forecast evaluation. The numerator can be viewed as a return which is inversely proportional to the MSE error of the forecasts, while the denominator can be viewed as the cost of forecasting which is inversely proportional to the MSE of the benchmark [@mincer1969]. Hence the ratio is representative of a rate of return index and ranks the performance of forecasts as such. Models that result in forecasts with a RM that is less than one are said to produce superior forecasts to the benchmark model being considered. 

# Conclusion
ARIMA models have been highly popular since its introduction by Box and Jenkins and are widely used for financial forecasting. They are easy to implement, have a systematic procedure for model design, and produce good forecasts over short time horizons. Although robust, they have the limitation of requiring large amounts of historical observations and they fail to capture the non-linearity inherent in exchange rate data. With the evolution of computational power and the advancement of statistics, more complex techniques such as ANN’s, Fuzzy Logistic Systems, and Hybrid ARIMA models have been proposed and compared to the traditional ARIMA model. Although these models produce superior results to the ARIMA model, they are less tractable and require the analyst to understand the complex statistical intricacies involved. Prophet’s Bayesian approach to forecasting is flexible and allows for non-experts to incorporate their domain knowledge in the model building process. If Prophet produces forecasts that are as good as the ARIMA model when forecasting Bitcoin/ZAR, it can be compared to these more complex forecasting methods that are less tractable and scalable.

\newpage

# References