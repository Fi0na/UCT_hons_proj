\documentclass[12pt,preprint, authoryear]{elsarticle}

\usepackage{lmodern}
%%%% My spacing
\usepackage{setspace}
\setstretch{1.5}
\DeclareMathSizes{12}{14}{10}{10}

% Wrap around which gives all figures included the [H] command, or places it "here". This can be tedious to code in Rmarkdown.
\usepackage{float}
\let\origfigure\figure
\let\endorigfigure\endfigure
\renewenvironment{figure}[1][2] {
    \expandafter\origfigure\expandafter[H]
} {
    \endorigfigure
}

\let\origtable\table
\let\endorigtable\endtable
\renewenvironment{table}[1][2] {
    \expandafter\origtable\expandafter[H]
} {
    \endorigtable
}


\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
    \usepackage{xltxtra,xunicode}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase}
  \newcommand{\euro}{â‚¬}
\fi

\usepackage{amssymb, amsmath, amsthm, amsfonts}

\usepackage[round]{natbib}
\bibliographystyle{natbib}
\def\bibsection{\section*{References}} %%% Make "References" appear before bibliography
\usepackage{longtable}
\usepackage[margin=2cm,bottom=4cm,top=2.5cm, includefoot]{geometry}
\usepackage{fancyhdr}
\usepackage[bottom, hang, flushmargin]{footmisc}
\usepackage{graphicx}
\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1.3ex plus 0.5ex minus 0.3ex}
\usepackage{textcomp}
\renewcommand{\headrulewidth}{0.2pt}
\renewcommand{\footrulewidth}{0.3pt}

\usepackage{array}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}

%%%%  Remove the "preprint submitted to" part. Don't worry about this either, it just looks better without it:
\makeatletter
\def\ps@pprintTitle{%
  \let\@oddhead\@empty
  \let\@evenhead\@empty
  \let\@oddfoot\@empty
  \let\@evenfoot\@oddfoot
}
\makeatother

 \def\tightlist{} % This allows for subbullets!

\usepackage{hyperref}
\hypersetup{breaklinks=true,
            bookmarks=true,
            colorlinks=true,
            citecolor=blue,
            urlcolor=blue,
            linkcolor=blue,
            pdfborder={0 0 0}}

\urlstyle{same}  % don't use monospace font for urls
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\setcounter{secnumdepth}{5}

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

%%% Include extra packages specified by user
% Insert custom packages here as follows
% \usepackage{tikz}

\begin{document}

\begin{frontmatter}  %

\title{Draft}

\author[Add1]{Fiona Ganie}
\ead{GNXFIO001@myuct.ac.za}





\address[Add1]{University of Cape Town}



\vspace{1cm}

\begin{keyword}
\footnotesize{
 \\ \vspace{0.3cm}
\textit{JEL classification} 
}
\end{keyword}
\vspace{0.5cm}
\end{frontmatter}



%________________________
% Header and Footers
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\chead{}
\rhead{}
\lfoot{}
\rfoot{\footnotesize Page \thepage\\}
\lhead{}
%\rfoot{\footnotesize Page \thepage\ } % "e.g. Page 2"
\cfoot{}

%\setlength\headheight{30pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%________________________

\headsep 35pt % So that header does not go over title




\section{\texorpdfstring{Introduction
\label{Introduction}}{Introduction }}\label{introduction}

The Autoregressive Integrated Moving Average (ARIMA) model is one of the
most widely used time series models that has attracted attention in
financial market forecasting (Kashei and Bijari 2007). The ARIMA model
is known for its attractive quality of being a rich class of processes,
making it possible to find a suitable model to adequately fit the data
(Fahimifard et al. \protect\hyperlink{ref-fahimifard2009}{2009}).
Although the Random Walk model has typically been applied to foreign
exchange markets and has produced superior results, some researchers
have contended that these markets are not efficient and believe that
future prices depend on current and past events (Abu-Mostafa and Atiya
\protect\hyperlink{ref-abu1996}{1996}). This has led to the application
of the ARIMA model to exchange rate problems, where it has since gained
popularity due to its ease of implementation and tractability. Although
the ARIMA model yields good forecasts over short forecast horizons and
is easy to use, the linearity of the ARIMA model fails to adequately
capture the non-linearity inherent in exchange rate data (Zhang and Hu
\protect\hyperlink{ref-zhang1998}{1998}).

With the evolution of computational power, non-linear, soft computing
techniques have been proposed as a solution. The ARIMA model has been
used as the standard benchmark model against which these more complex
methods have been compared. Although they have produced significantly
better results than the ARIMA model, these models lack interpretability
and building them is a challenging task. The ARIMA model is more
tractable, less computationally expensive, has been used as the building
blocks for more advanced models and has provided the inspiration for
hybrid versions of the model which have been used in exchange rate
forecasting.

In December 2016, Facebook open-sourced their forecasting model Prophet.
Prophet differs to traditional time series models like ARIMA, in that it
can be utilised by non-experts who have little knowledge about the
statistical intricacies involved in the model, however have domain
knowledge about the data generating process. This can then be easily
incorporated into the model through intuitively adjustable parameters
(Taylor and Letham \protect\hyperlink{ref-taylor2017}{2017}). If Prophet
produces forecasts that are as good as the ARIMA model, it can be
compared to more complex forecasting methods that are less tractable and
flexible.

This paper broadly aims to compare the forecasts produced by the
traditional ARIMA model and Prophet through an evaluation of
Bitcoin/ZAR, using a Mincer-Zarnowitz approach of measuring forecast
accuracy. More specifically, this paper aims to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Compare the out-of-sample forecasts over varying forecast horizons.
\item
  Compare the forecasts produced by ARIMA's and Prophet's automatic
  selection procedures.
\item
  Compare the out-of-sample forecast performance with missing values and
  outliers present in the data.
\item
  Investigate the efficient market hypothesis (EMH) and the trading
  profit to be made with Prophet.
\end{enumerate}

Section \ref{Background} will present the key findings from the major
works in which comparisons have been made to the ARIMA model in exchange
rate forecasting, as well as the theory behind the ARIMA and Prophet
model. This will be followed by a brief explanation and motivation for
the dataset used in this paper in Section \ref{Data}. Section
\ref{Methodology} will describe the methodology used in selecting the
ARIMA and Prophet models and the Mincer-Zarnowitz approach of measuring
forecast accuracy. The results from applying the ARIMA and Prophet model
to the dataset is presented in Section \ref{Results}. Finally, Section
\ref{Discussion and Conclusions} discusses the results in a statistical
and financially meaningful context.

\section{\texorpdfstring{Background
\label{Background}}{Background }}\label{background}

\subsection{Forecasting Exchange Rates with the ARIMA
model}\label{forecasting-exchange-rates-with-the-arima-model}

ARIMA models became highly popular since its introduction by Box and
Jenkins, when it was shown that they could outperform complex
econometric models in a variety of situations (Hibon and Makridakis
\protect\hyperlink{ref-spyros1997}{1997}). The ARIMA model expresses the
process \{\(y_t\)\} as a function of the weighted average of past values
of the process and lagged values of the residuals. The weighted average
of the past p values of the process represents an autoregressive (AR)
process of order p.~It feeds back past values of the process into the
current value, inducing correlation between all lags of the process. The
weighted average of the q lagged residuals represents a moving average
(MA) process of order q. The purpose of mixing the MA process with the
AR process is to reduce the large number of past values required by AR
processes and to control for the autocorrelation which it creates
between lagged values of the process. The combination of the AR(p) and
MA(q) process results in a more parsimonious model, and forms a
stationary autoregressive moving average (ARMA(p,q)) process defined as:

\[ 
    y_t = c + \sum_{i = 1}^{p} \theta_{i} y_{t-i} + \sum_{i = 1}^{q} \phi_{i} e_{t-i}+ e_t \label{eq1} \\ \notag
\] where c is a constant, \(\theta\) and \(\phi\) are parameters set
such that \ldots{} and \{\(e_t\)\} is a white noise process with zero
mean and variance \(\sigma^2\).

The ARIMA(p,d,q) model generalises the ARMA model in that it includes
both stationary and non-stationary processes. The parameter d is the
degree of differencing required to render the process stationary. If d
is equal to zero the process is stationary and equivalent to an ARMA
model, and if d is strictly positive the process requires differencing
to make it stationary. The ARIMA model can be defined succinctly using
the backward shift operator B, which shifts the process back by one unit
of time, and is defined as \(By_t = y_{t-1}\). The ARIMA model has the
form:

\[ 
    (1-\sum_{i = 1}^{p} \theta_{i} B^i)(1-B)^dy_t = c + (1 + \sum_{i = 1}^{q} \phi_{i} B^i)e_t \label{eq2} \\ \notag 
\] where c is a constant and \{\(e_t\)\} is a white noise process with
zero mean and variance \(\sigma^2\) (otext.org/fpp/8/5)\ldots{}need to
get this reference hmmm

ARIMA models have been commonly used in financial forecasting and are
popular for observing stock prices and exchange rates due to its power
and statistical properties (C.-S. Lin, Chiu, and Lin
\protect\hyperlink{ref-lin2012}{2012}). They have frequently been used
as a benchmark to compare new forecasting techniques that have emerged
over time. Kamruzzaman and Sarker
(\protect\hyperlink{ref-kamruzzaman2003}{2003}) applied an ARIMA model
to forecast the exchange rate between the Australian dollar and six
other currencies, and used the forecast errors as well as the accuracy
of the direction of the forecasts to evaluate the performance of three
Neural Network models. In a similar study, Khashei, Bijari, and Ardali
(\protect\hyperlink{ref-khashei2012}{2012}) compared the predictive
capability of their proposed hybrid model consisting of an ARIMA and a
Probabilistic Neural Network (PNN), to the traditional ARIMA model, and
justified the use of the ARIMA as a benchmark by claiming that it is the
most important linear model. Their comparison was made by investigating
the forecasts between the British pound and US dollar over varying
forecast horizons. Not only has the ARIMA model been useful as a
benchmark, but it has also yielded satisfactory results when predicting
exchange rates. When Nwankwo (\protect\hyperlink{ref-nwankwo2014}{2014})
forecasted the rate between the Nairo and dollar, diagnostic testing
revealed that the ARIMA(1,0,0) model was the best fit for the data based
on Akaike's Information Criterion (AIC).

The ARIMA model is attractive as it is tractable and produces good
short-term forecasts when more than 100 observations are used (Tseng et
al. \protect\hyperlink{ref-tseng2001}{2001}). Although the ARIMA model
has the advantage of ease of implementation and flexibility, it fails to
capture the non-linearity and volatility present in exchange rate data.
Over time, the ARIMA model has evolved to cater for a wider variety of
data and to compensate for some of its shortcomings. The most popular
versions of the ARIMA model that has been implemented in exchange rate
forecasting is the Seasonal ARIMA (SARIMA) and Fractional ARIMA model
(FARIMA).

The SARIMA model was introduced to capture the periodic behaviour of
data and extends the ARIMA class by including a term for seasonal
differencing. Etuk, Wokoma, and Moffat
(\protect\hyperlink{ref-etuk2013}{2013}) modelled the Naira/CFA Franc
exchange rate which exhibited monthly seasonality using an additive
SARIMA model, to demonstrate that it can be a useful fit for exchange
rate data which displays seasonality. Their results showed that the
SARIMA model adequately described the variation in the exchange rate
series. The ARFIMA model generalises the ARIMA model in that the degree
required to make the data stationary can assume any real value, and is
no longer restricted to the integer domain. The ARFIMA model has the
ability to capture the dependence between observations that are widely
spread apart in time (Cheung \protect\hyperlink{ref-cheung1993}{1993}).
This makes the model more parsimonious since it can capture long memory
in data as well as short term dynamics (Cheung
\protect\hyperlink{ref-cheung1993}{1993}). Cheung
(\protect\hyperlink{ref-cheung1993}{1993}) fitted the ARFIMA model to
examine five exchange rates, and found that there was strong evidence of
long memory in the exchange rate time series.

Generalised autoregressive conditional heteroskedasticity (GARCH) models
were later developed in an attempt to capture the volatility in
financial markets (Anastasakis and Mort
\protect\hyperlink{ref-anastasakis2009}{2009}). Hsieh (1989) applied a
GARCH model to investigate five exchange rates and his results showed
that although the GARCH model outperformed the random walk, some
non-linear information still remained in the residuals.

\subsection{Other techniques used to forecast exchange
rates}\label{other-techniques-used-to-forecast-exchange-rates}

Over time, financial forecasting methods have moved away from linear
models like ARIMA and GARCH, to soft computing techniques. These more
complex techniques are non-linear and can fit complex time series more
easily (Castillo and Melin \protect\hyperlink{ref-castillo2002}{2002}).
Unlike the ARIMA model, soft computing techniques do not impose
structural assumptions on the model apriori (Castillo and Melin
\protect\hyperlink{ref-castillo2002}{2002}). Some of the most commonly
used artificial intelligence methods used to forecast exchange rate data
are Neural Networks and Fuzzy Logistic Systems.

Artificial Neural Networks (ANNs) have had many successful applications
in forecasting exchange rates and are more advantageous than other
non-linear forecasting methods. They are data driven, can adapt to
non-stationary environments and can approximate any continuous function
(Khashei and Bijari \protect\hyperlink{ref-khashei2011}{2011}). In a
study done by Fahimifard et al.
(\protect\hyperlink{ref-fahimifard2009}{2009}), the ANN was found as an
effective way to improve the forecasts of exchange rates. Superior
results were produced when compared to the ARIMA and GARCH model using
the root mean square error (RMSE), mean square error (MSE) and mean
absolute difference as a measure of performance. Franses and Van Homelen
(\protect\hyperlink{ref-franses1998}{1998}) suggested that the
non-linear feature of exchange rates that are picked up by ANNs may
actually be due to neglected GARCH effects. They investigated the
application of ANNs to forecast exchange rates and found that the
presence of GARCH in the data may mistakenly lead one to believe that
the returns can be forecasted on the exchange rates themselves. Their
results show that there is no gain in producing out-of-sample forecasts
using ANNs if the data is not truly non-linear. Fuzzy Logistic Systems
(FLSs) were initially developed to solve problems involving linguistic
terms, and have been successfully used in financial forecasting
(Khashei, Bijari, and Ardali \protect\hyperlink{ref-khashei2009}{2009}).
Fuzzy logic tries to imitate human reasoning and the decision-making
process and allows for finer rather discrete decisions to be provided.
Santos, Costa, and Santos Coelho
(\protect\hyperlink{ref-santos2007}{2007}) investigated how well FLSs
and ANNs perform compared to the traditional ARMA and GARCH model. He
examined the forecasts of Brazilian exchange rate returns by considering
different frequencies of the series and comparing their one step-ahead
forecasts. By analysing the RMSE, U-Theil inequality index, percentage
of corrected predicted signals (CPS), and the Pesaran-Timmermann (PT)
predictive failure statistic, he found that FLSs and ANNs achieved
higher returns based on the forecasts they produced. Similar results
were found by Khashei, Bijari, and Ardali
(\protect\hyperlink{ref-khashei2009}{2009}) when he analysed the
predictive capabilities of FLSs, ANNs, the traditional ARIMA model, and
a Fuzzy ARIMA model.

Although ANNs have been broadly applied in financial forecasting, the
process of building them is a complex task and there is no consistent
method of design compared to the traditional Box-Jenkins ARIMA model.
Unlike ARIMA models, the performance of ANNs is sensitive to many
modelling factors such as the number of input nodes included and the
size of the training sample chosen (Zhang and Hu
\protect\hyperlink{ref-zhang1998}{1998}). Like ANNs there is no
systematic approach to designing FLSs and they are only understandable
when simple. Although FLSs has the advantage over ARIMA models that they
can be applied to data with few observations available, it gives
acceptable rather than accurate results and are more suitable for
problems which do not require high accuracy.

\subsection{Hybrid ARIMA Models}\label{hybrid-arima-models}

Over time, many researchers began to think of ways in which to harness
the advantages of tractable linear models such as the ARIMA model, and
of more complex non-linear models. By combining the ARIMA model with
other forecasting methods, the advantages of both forecasting methods
are leveraged while simultaneously improving their limitations. Some of
the hybrid models which have commonly been used in exchange rate
forecasting are the Fuzzy ARIMA and ANN-ARIMA model.

The ARIMA model produces very accurate forecasts over short time
horizons however it has the limitation of requiring more than 100
observations of historical data to yield accurate results (Tseng et al.
\protect\hyperlink{ref-tseng2001}{2001}). In a world that is constantly
changing and with the rapid advancement of technology, access to large
amounts of historical data is difficult to obtain. On the contrary, a
fuzzy regression model requires little historical data however produces
wide prediction intervals if extreme values are present in the data. The
Fuzzy ARIMA model combines the ARIMA and fuzzy regression model to
exploit the advantages of both models while simultaneously overcoming
their limitations. Tseng et al.
(\protect\hyperlink{ref-tseng2001}{2001}) proposed applying a Fuzzy
ARIMA model to forecast the exchange rate of Taiwan dollars to US
dollars to demonstrate the model's appropriateness and power. The Fuzzy
ARIMA not only produced forecasts that were superior to the ARIMA and
fuzzy time series models but also provided an upper and lower bound
which can be used by decision makers to determine the best and worst
possible situations.

Although ARIMA models are powerful, they require non-stationary data to
be differenced and impose prior assumptions onto the distribution of the
data (Ince and Trafalis \protect\hyperlink{ref-ince2006}{2006}). In
contrast, machine learning techniques such as ANNs do not impose any
assumptions onto the data generating process however, being data-driven,
are sensitive to the number of input nodes used. The ANN-ARIMA model
draws on the strengths of both models and overcomes these individual
difficulties. Ince and Trafalis (\protect\hyperlink{ref-ince2006}{2006})
created an ANN-ARIMA model by using the ARIMA model to determine the
number input nodes required by an ANN for three exchange rates. When the
ANN-ARIMA model was compared to the pure ARIMA model, the hybrid model
outperformed the ARIMA based on the MSE. The findings of Khashei and
Bijari (\protect\hyperlink{ref-khashei2011}{2011}) study agree with
these results. They implemented an ARIMA model and used its residuals
together with past observations of the data as inputs for the ANN. Their
hybrid model had superior in-sample and out-sample forecasts compared to
the random walk, linear autoregressive and ANN model.

Although the traditional ARIMA models produce less superior forecasts
than its hybrid forms and other complex non-linear techniques, its
forecasts are still satisfactory. They are simple models that are easy
to implement and have a consistent method of model design and selection.
ARIMA models are also more robust and efficient than complex structural
models in relation to short-run forecasting. The fact that they have
been used as the foundation for more advanced models and have commonly
been used as a benchmark for comparison justifies it as a good starting
point to compare it to Facebook's forecasting method, Prophet, that was
recently released.

\subsection{Forecasting with Prophet}\label{forecasting-with-prophet}

The techniques that have been considered for exchange rate forecasting
thus far, require the analyst to have vocational knowledge about time
series. Prophet differs to traditional time series models in that it is
flexible and can be customised by a large number of non-experts who have
little knowledge about time series, however have domain knowledge about
the data generating process. Prophet allows for a large number of
forecasts to be produced across a variety of problems and consists of a
robust evaluation system that allows for a large number of forecasts be
evaluated and compared. This is Facebook's definition of forecasting at
scale.

Prophet consists of a decomposable model of the form:

\[ 
    y(t) = g(t) + s(t) + h(t) + e_t
\] where the components of the model represent the growth, seasonality
and holiday respectively, and \(e_t\) is white noise.

These components consist of linear and non-linear functions of time.
This differs to ARIMA models in which future values of the process are
linear functions of previous observations and lagged residuals. Prophet
is more like a Generalized Additive Model (GAM) which is a regression
model that consists of non-linear and linear regression functions
applied to predictor variables (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). Prophet, like the GAM, frames
the forecasting problem as a curve fitting exercise, using backfitting
to find the regression functions. The GAM is fitted quickly, allowing
the analyst to interactively change the model parameters (Taylor and
Letham \protect\hyperlink{ref-taylor2017}{2017}).

The growth component is modelled in a similar way to population growths
which use a logistic growth model (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). Populations typically grow
non-linearly (although the growth component could also be linear) up to
an upper bound known as the carrying capacity, and remains constant
thereafter. The rate at which the population grows changes over time,
and this is accounted for by including changepoints in the model where
the growth rate can be automatically selected and may be adjusted
(Taylor and Letham \protect\hyperlink{ref-taylor2017}{2017}). This
allows non-experts with knowledge about events that may affect growth to
use the parameter as a knob and adjust it to increase or decrease the
number of changepoints (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). It also allows for the
analyst to add changepoints which the automatic selection procedure may
have missed (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). Furthermore, analysts may
also specify the carrying capacity and adjust it based on their
knowledge of the total market size (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}).

The decomposable form of the model allows for components to be easily
added to it. This allows for multiple seasonality components with
different periods to be added to the model. The variance of the
parameters of the seasonality component's distribution can be adjusted
by analysts to smooth the model and change how much of historical
seasonality is projected to the future (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}).

The name, date, and country of past and future holidays and events may
also be inputted by the analyst into a list (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). By specifying the country in
which the events take place or the holidays occur, separate lists can be
populated for global events/holidays and country-specific
events/holidays. The union of the two lists are then used for
forecasting. Like seasonality, the variance of the parameters of the
holiday component's distribution, can be adjusted by analysts to smooth
the model (Taylor and Letham \protect\hyperlink{ref-taylor2017}{2017}).

\section{\texorpdfstring{Data \label{Data}}{Data }}\label{data}

\section{\texorpdfstring{Methodology
\label{Methodology}}{Methodology }}\label{methodology}

\subsection{The Box-Jenkins Methodology and Automatic
Selection}\label{the-box-jenkins-methodology-and-automatic-selection}

Box and Jenkins proposed a set of guidelines that can be followed when
selecting ARIMA models. This systematic procedure to designing ARIMA
models has made them highly popular (Hibon and Makridakis
\protect\hyperlink{ref-spyros1997}{1997}). It consists of a four-stage
iterative process in which: 1) the process is either transformed or
differenced to detrend and stabilise the variance of the data, 2) the
autocorrelation and partial autocorrelation plots are used to determine
the order of p and q, 3) the parameters of the model are estimated, 4) a
diagnostic check is performed to ensure that the residuals are a white
noise process. If the residuals are not white noise steps 2-4 are
repeated until a satisfactory model is identified. On the contrary, if
the diagnostic check shows that the residuals are random then the
developed model is the final model used for forecasting.

Due to the large number of forecasts made, it is useful to have an
automatic procedure which is able to select the appropriate ARIMA model
to fit the data. The auto.arima function in R is able to automatically
choose the order of the parameters p, q and d. The order of differencing
is determined by using the KPSS test (Ruppert and Matteson
\protect\hyperlink{ref-ruppert2015}{2015}). The KPSS test checks the
null hypothesis of stationarity and sets d to zero if the null
hypothesis is accepted, otherwise it iteratively increases d by 1 and
tests the null hypothesis until it is accepted (Ruppert and Matteson
\protect\hyperlink{ref-ruppert2015}{2015}). Once the order of
differencing has been determined, the order of p and q are chosen based
on Akaike's Information Criterion (AIC) or the Bayesian Information
Criterion (BIC) (Ruppert and Matteson
\protect\hyperlink{ref-ruppert2015}{2015}).

\subsection{Semi-Automatic Selection}\label{semi-automatic-selection}

When a large number of forecasts are produced, manually identifying
problematic forecasts becomes a time consuming and difficult task.
Prophet provides a semi-automated forecast evaluation system that
selects the best model that fits the data. When there are large forecast
errors, the forecasts are flagged so that the analyst can explore the
cause of the errors, identify and remove potential outliers and either
adjust the model or choose a more appropriate model (Taylor and Letham
\protect\hyperlink{ref-taylor2017}{2017}). Unlike ARIMA's
fully-automated evaluation system, Prophet provides interactive feedback
and keeps the analyst in the loop.

\subsection{Mincer-Zarnowitz Approach to Forecast
Evaluation}\label{mincer-zarnowitz-approach-to-forecast-evaluation}

The Mincer-Zarnowitz approach to evaluating forecast accuracy is
commonly used and can be useful when comparing the forecasts produced by
Prophet and the ARIMA model. Mincer and Zarnowitz
(\protect\hyperlink{ref-mincer1969}{1969}) proposed an absolute and
relative measure to evaluate forecast accuracy. Absolute measures
consider the distance between actual and predicted values. To analyse
the absolute errors produced by the forecasts, the observed values are
regressed against the predicted values. The intercept of the regression
equation represents the mean distance between the observed and predicted
values while the slope represents the correlation between the residual
errors and the predicted values. A zero intercept implies that the
forecasts are unbiased and do not overestimate or underestimate the
data, while a unity slope implies that the forecasts are efficient and
uncorrelated with the residual errors. A joint hypothesis test is
performed to check this efficiency and bias of the forecasts, and models
which produce the best results are selected.

Absolute forecast measures cannot be used to make comparisons between
forecasts with different scales or economic variables. Furthermore, the
size of forecasting errors is not as significant as the consequences of
forecasting errors and how they impact the decision making process.
Hence, rather than using absolute measures of forecast accuracy, Mincer
and Zarnowitz (\protect\hyperlink{ref-mincer1969}{1969}) suggested the
use of relative measures. Relative accuracy analysis allows for
meaningful comparisons of different forecasting methods to be made. It
uses an index which considers the ratio of the MSE of the forecast to
the MSE of a benchmark forecasting method. A useful benchmark that may
be used is an extrapolation of the data history, as it is a cost
effective and accessible method, however any method which is relevant
for comparison may be used. This ratio is known as the Relative Mean
Square Error (RM) for forecast evaluation. The numerator can be viewed
as a return which is inversely proportional to the MSE error of the
forecasts, while the denominator can be viewed as the cost of
forecasting which is inversely proportional to the MSE of the benchmark
(Mincer and Zarnowitz \protect\hyperlink{ref-mincer1969}{1969}). Hence
the ratio is representative of a rate of return index and ranks the
performance of forecasts as such. Models that result in forecasts with a
RM that is less than one are said to produce superior forecasts to the
benchmark model being considered.

\section{\texorpdfstring{Results
\label{Results}}{Results }}\label{results}

\section{\texorpdfstring{Discussion and Conclusions
\label{Discussion and Conclusions}}{Discussion and Conclusions }}\label{discussion-and-conclusions}

\newpage

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\hypertarget{refs}{}
\hypertarget{ref-abu1996}{}
Abu-Mostafa, Yaser S, and Amir F Atiya. 1996. ``Introduction to
Financial Forecasting.'' \emph{Applied Intelligence} 6 (3). Springer:
205--13.

\hypertarget{ref-anastasakis2009}{}
Anastasakis, Leonidas, and Neil Mort. 2009. ``Exchange Rate Forecasting
Using a Combined Parametric and Nonparametric Self-Organising Modelling
Approach.'' \emph{Expert Systems with Applications} 36 (10). Elsevier:
12001--11.

\hypertarget{ref-castillo2002}{}
Castillo, Oscar, and Patricia Melin. 2002. ``Hybrid Intelligent Systems
for Time Series Prediction Using Neural Networks, Fuzzy Logic, and
Fractal Theory.'' \emph{IEEE Transactions on Neural Networks} 13 (6).
IEEE: 1395--1408.

\hypertarget{ref-cheung1993}{}
Cheung, Yin-Wong. 1993. ``Long Memory in Foreign-Exchange Rates.''
\emph{Journal of Business \& Economic Statistics} 11 (1). Taylor \&
Francis: 93--101.

\hypertarget{ref-etuk2013}{}
Etuk, Ette Harrison, Dagogo SA Wokoma, and Imoh Udo Moffat. 2013.
``Additive Sarima Modelling of Monthly Nigerian Naira-Cfa Franc Exchange
Rates.'' \emph{European Journal of Statistics and Probability} 1 (1):
1--12.

\hypertarget{ref-fahimifard2009}{}
Fahimifard, SM, Masuod Homayounifar, M Sabouhi, and AR Moghaddamnia.
2009. ``Comparison of Anfis, Ann, Garch and Arima Techniques to Exchange
Rate Forecasting.'' \emph{Journal of Applied Sciences} 9 (20): 3641--51.

\hypertarget{ref-franses1998}{}
Franses, Philip Hans, and Paul Van Homelen. 1998. ``On Forecasting
Exchange Rates Using Neural Networks.'' \emph{Applied Financial
Economics} 8 (6). Taylor \& Francis: 589--96.

\hypertarget{ref-spyros1997}{}
Hibon, Michael, and Spyros Makridakis. 1997. ``ARMA Models and the
Box--Jenkins Methodology.'' John Wiley \& Sons, Ltd.

\hypertarget{ref-ince2006}{}
Ince, Huseyin, and Theodore B Trafalis. 2006. ``A Hybrid Model for
Exchange Rate Prediction.'' \emph{Decision Support Systems} 42 (2).
Elsevier: 1054--62.

\hypertarget{ref-kamruzzaman2003}{}
Kamruzzaman, Joarder, and Ruhul A Sarker. 2003. ``Forecasting of
Currency Exchange Rates Using Ann: A Case Study.'' In \emph{Neural
Networks and Signal Processing, 2003. Proceedings of the 2003
International Conference on}, 1:793--97. IEEE.

\hypertarget{ref-khashei2011}{}
Khashei, Mehdi, and Hehdi Bijari. 2011. ``Exchange Rate Forecasting
Better with Hybrid Artificial Neural Networks Models.'' \emph{Journal of
Mathematical and Computational Science} 1 (1). Science \& Knowledge
Publishing Corporation Limited (SCIK): 103.

\hypertarget{ref-khashei2009}{}
Khashei, Mehdi, Mehdi Bijari, and Gholam Ali Raissi Ardali. 2009.
``Improvement of Auto-Regressive Integrated Moving Average Models Using
Fuzzy Logic and Artificial Neural Networks (Anns).''
\emph{Neurocomputing} 72 (4). Elsevier: 956--67.

\hypertarget{ref-khashei2012}{}
---------. 2012. ``Hybridization of Autoregressive Integrated Moving
Average (Arima) with Probabilistic Neural Networks (Pnns).''
\emph{Computers \& Industrial Engineering} 63 (1). Elsevier: 37--45.

\hypertarget{ref-lin2012}{}
Lin, Chiun-Sin, Sheng-Hsiung Chiu, and Tzu-Yu Lin. 2012. ``Empirical
Mode Decomposition--based Least Squares Support Vector Regression for
Foreign Exchange Rate Forecasting.'' \emph{Economic Modelling} 29 (6).
Elsevier: 2583--90.

\hypertarget{ref-mincer1969}{}
Mincer, Jacob A, and Victor Zarnowitz. 1969. ``The Evaluation of
Economic Forecasts.'' In \emph{Economic Forecasts and Expectations:
Analysis of Forecasting Behavior and Performance}, 3--46. NBER.

\hypertarget{ref-nwankwo2014}{}
Nwankwo, Steve C. 2014. ``Autoregressive Integrated Moving Average
(Arima) Model for Exchange Rate (Naira to Dollar).'' \emph{Academic
Journal of Interdisciplinary Studies} 3 (4): 429.

\hypertarget{ref-ruppert2015}{}
Ruppert, David, and David S Matteson. 2015. \emph{Statistics and Data
Analysis for Financial Engineering: With R Examples}. Springer.

\hypertarget{ref-santos2007}{}
Santos, AndrÃ© Alves Portela, Newton Carneiro Affonso da Costa, and
Leandro dos Santos Coelho. 2007. ``Computational Intelligence Approaches
and Linear Models in Case Studies of Forecasting Exchange Rates.''
\emph{Expert Systems with Applications} 33 (4). Elsevier: 816--23.

\hypertarget{ref-taylor2017}{}
Taylor, Sean J, and Benjamin Letham. 2017. ``Forecasting at Scale.''

\hypertarget{ref-tseng2001}{}
Tseng, Fang-Mei, Gwo-Hshiung Tzeng, Hsiao-Cheng Yu, and Benjamin JC
Yuan. 2001. ``Fuzzy Arima Model for Forecasting the Foreign Exchange
Market.'' \emph{Fuzzy Sets and Systems} 118 (1). Elsevier: 9--19.

\hypertarget{ref-zhang1998}{}
Zhang, Gioqinang, and Michael Y Hu. 1998. ``Neural Network Forecasting
of the British Pound/Us Dollar Exchange Rate.'' \emph{Omega} 26 (4).
Elsevier: 495--506.

% Force include bibliography in my chosen format:
\newpage
\nocite{*}
\bibliography{}





\end{document}
