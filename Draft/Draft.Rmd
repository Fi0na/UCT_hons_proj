---
title: "Draft"

documentclass: "elsarticle"

Author1: "Fiona Ganie"  
Ref1: "University of Cape Town" 
Email1: "GNXFIO001\\@myuct.ac.za" 

BottomRFooter: "\\footnotesize Page \\thepage\\" 
addtoprule: TRUE
addfootrule: TRUE 
bibliography: Tex/ref.bib  
RemovePreprintSubmittedTo: TRUE

toc: no                        
numbersections: yes             

fontsize: 12pt                  
linestretch: 1.5                
link-citations: TRUE          

output:
  pdf_document:
    keep_tex: TRUE
    template: Tex/TexDefault.txt
    fig_width: 3.5 
    fig_height: 3.5
    include:
      in_header: Tex/packages.txt 
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 6, fig.height = 5, fig.pos="H", fig.pos = 'H')

```

# Introduction \label{Introduction}

The Autoregressive Integrated Moving Average (ARIMA) model is one of the most widely used time series models that has attracted attention in financial market forecasting (Kashei and Bijari 2007). The ARIMA model is known for its attractive quality of being a rich class of processes, making it possible to find a suitable model to adequately fit the data [@fahimifard2009]. Although the Random Walk model has typically been applied to foreign exchange markets and has produced superior results, some researchers have contended that these markets are not efficient and believe that future prices depend on current and past events [@abu1996]. This has led to the application of the ARIMA model to exchange rate problems, where it has since gained popularity due to its ease of implementation and tractability. Although the ARIMA model yields good forecasts over short forecast horizons and is easy to use, the linearity of the ARIMA model fails to adequately capture the non-linearity inherent in exchange rate data [@zhang1998]. 

With the evolution of computational power, non-linear, soft computing techniques have been proposed as a solution. The ARIMA model has been used as the standard benchmark model against which these more complex methods have been compared. Although they have produced significantly better results than the ARIMA model, these models lack interpretability and building them is a challenging task. The ARIMA model is more tractable, less computationally expensive, has been used as the building blocks for more advanced models and has provided the inspiration for hybrid versions of the model which have been used in exchange rate forecasting. 

In December 2016, Facebook open-sourced their forecasting model Prophet. Prophet differs to traditional time series models like ARIMA, in that it can be utilised by non-experts who have little knowledge about the statistical intricacies involved in the model, however have domain knowledge about the data generating process. This can then be easily incorporated into the model through intuitively adjustable parameters [@taylor2017]. If Prophet produces forecasts that are as good as the ARIMA model, it can be compared to more complex forecasting methods that are less tractable and flexible. 

This paper broadly aims to compare the forecasts produced by the traditional ARIMA model and Prophet through an evaluation of Bitcoin/ZAR, using a Mincer-Zarnowitz approach of measuring forecast accuracy. More specifically, this paper aims to: 

1) Compare the out-of-sample forecasts over varying forecast horizons.

2) Compare the forecasts produced by ARIMA's and Prophet's automatic selection procedures.

3) Compare the out-of-sample forecast performance with missing values and outliers present in the data. 

4) Investigate the efficient market hypothesis (EMH) and the trading profit to be made with Prophet. 
 
Section \ref{Background} will present the key findings from the major works in which comparisons have been made to the ARIMA model in exchange rate forecasting, as well as the theory behind the ARIMA and Prophet model. This will be followed by a brief explanation and motivation for the dataset used in this paper in Section \ref{Data}. Section \ref{Methodology} will describe the methodology used in selecting the ARIMA and Prophet models and the Mincer-Zarnowitz approach of measuring forecast accuracy. The results from applying the ARIMA and Prophet model to the dataset is presented in Section \ref{Results}. Finally, Section \ref{Discussion and Conclusions} discusses the results in a statistical and financially meaningful context.  

# Background \label{Background}

## Forecasting Exchange Rates with the ARIMA model
ARIMA models became highly popular since its introduction by Box and Jenkins, when it was shown that they could outperform complex econometric models in a variety of situations [@spyros1997]. The ARIMA model expresses the process {$y_t$} as a function of the weighted average of past values of the process and lagged values of the residuals. The weighted average of the past p values of the process represents an autoregressive (AR) process of order p. It feeds back past values of the process into the current value, inducing correlation between all lags of the process. The weighted average of the q lagged residuals represents a moving average (MA) process of order q. The purpose of mixing the MA process with the AR process is to reduce the large number of past values required by AR processes and to control for the autocorrelation which it creates between lagged values of the process. The combination of the AR(p) and MA(q) process results in a more parsimonious model, and forms a stationary autoregressive moving average (ARMA(p,q)) process defined as:

$$ 
	y_t = c + \sum_{i = 1}^{p} \theta_{i} y_{t-i} + \sum_{i = 1}^{q} \phi_{i} e_{t-i}+ e_t \label{eq1} \\ \notag
$$
where c is a constant, $\theta$ and $\phi$ are parameters set such that ... and {$e_t$} is a white noise process with zero mean and variance $\sigma^2$.

The ARIMA(p,d,q) model generalises the ARMA model in that it includes both stationary and non-stationary processes. The parameter d is the degree of differencing required to render the process stationary. If d is equal to zero the process is stationary and equivalent to an ARMA model, and if d is strictly positive the process requires differencing to make it stationary. The ARIMA model can be defined succinctly using the backward shift operator B, which shifts the process back by one unit of time, and is defined as $By_t = y_{t-1}$. The ARIMA model has the form:
 
$$ 
	(1-\sum_{i = 1}^{p} \theta_{i} B^i)(1-B)^dy_t = c + (1 + \sum_{i = 1}^{q} \phi_{i} B^i)e_t \label{eq2} \\ \notag 
$$
where c is a constant and {$e_t$} is a white noise process with zero mean and variance $\sigma^2$ (otext.org/fpp/8/5)...need to get this reference hmmm

ARIMA models have been commonly used in financial forecasting and are popular for observing stock prices and exchange rates due to its power and statistical properties [@lin2012]. They have frequently been used as a benchmark to compare new forecasting techniques that have emerged over time. @kamruzzaman2003 applied an ARIMA model to forecast the exchange rate between the Australian dollar and six other currencies, and used the forecast errors as well as the accuracy of the direction of the forecasts to evaluate the performance of three Neural Network models. In a similar study, @khashei2012 compared the predictive capability of their proposed hybrid model consisting of an ARIMA and a Probabilistic Neural Network (PNN), to the traditional ARIMA model, and justified the use of the ARIMA as a benchmark by claiming that it is the most important linear model. Their comparison was made by investigating the forecasts between the British pound and US dollar over varying forecast horizons. Not only has the ARIMA model been useful as a benchmark, but it has also yielded satisfactory results when predicting exchange rates. When @nwankwo2014 forecasted the rate between the Nairo and dollar, diagnostic testing revealed that the ARIMA(1,0,0) model was the best fit for the data based on Akaike's Information Criterion (AIC).

The ARIMA model is attractive as it is tractable and produces good short-term forecasts when more than 100 observations are used [@tseng2001]. Although the ARIMA model has the advantage of ease of implementation and flexibility, it fails to capture the non-linearity and volatility present in exchange rate data. Over time, the ARIMA model has evolved to cater for a wider variety of data and to compensate for some of its shortcomings. The most popular versions of the ARIMA model that has been implemented in exchange rate forecasting is the Seasonal ARIMA (SARIMA) and Fractional ARIMA model (FARIMA). 

The SARIMA model was introduced to capture the periodic behaviour of data and extends the ARIMA class by including a term for seasonal differencing. @etuk2013 modelled the Naira/CFA Franc exchange rate which exhibited monthly seasonality using an additive SARIMA model, to demonstrate that it can be a useful fit for exchange rate data which displays seasonality. Their results showed that the SARIMA model adequately described the variation in the exchange rate series. The ARFIMA model generalises the ARIMA model in that the degree required to make the data stationary can assume any real value, and is no longer restricted to the integer domain. The ARFIMA model has the ability to capture the dependence between observations that are widely spread apart in time [@cheung1993]. This makes the model more parsimonious since it can capture long memory in data as well as short term dynamics [@cheung1993]. @cheung1993 fitted the ARFIMA model to examine five exchange rates, and found that there was strong evidence of long memory in the exchange rate time series. 

Generalised autoregressive conditional heteroskedasticity (GARCH) models were later developed in an attempt to capture the volatility in financial markets [@anastasakis2009]. Hsieh (1989) applied a GARCH model to investigate five exchange rates and his results showed that although the GARCH model outperformed the random walk, some non-linear information still remained in the residuals. 

## Other techniques used to forecast exchange rates
Over time, financial forecasting methods have moved away from linear models like ARIMA and GARCH, to soft computing techniques. These more complex techniques are non-linear and can fit complex time series more easily [@castillo2002]. Unlike the ARIMA model, soft computing techniques do not impose structural assumptions on the model apriori [@castillo2002]. Some of the most commonly used artificial intelligence methods used to forecast exchange rate data are Neural Networks and Fuzzy Logistic Systems. 

Artificial Neural Networks (ANNs) have had many successful applications in forecasting exchange rates and are more advantageous than other non-linear forecasting methods. They are data driven, can adapt to non-stationary environments and can approximate any continuous function [@khashei2011]. In a study done by @fahimifard2009, the ANN was found as an effective way to improve the forecasts of exchange rates. Superior results were produced when compared to the ARIMA and GARCH model using the root mean square error (RMSE), mean square error (MSE) and mean absolute difference as a measure of performance. @franses1998 suggested that the non-linear feature of exchange rates that are picked up by ANNs may actually be due to neglected GARCH effects. They investigated the application of ANNs to forecast exchange rates and found that the presence of GARCH in the data may mistakenly lead one to believe that the returns can be forecasted on the exchange rates themselves. Their results show that there is no gain in producing out-of-sample forecasts using ANNs if the data is not truly non-linear. Fuzzy Logistic Systems (FLSs) were initially developed to solve problems involving linguistic terms, and have been successfully used in financial forecasting [@khashei2009]. Fuzzy logic tries to imitate human reasoning and the decision-making process and allows for finer rather discrete decisions to be provided. @santos2007 investigated how well FLSs and ANNs perform compared to the traditional ARMA and GARCH model. He examined the forecasts of Brazilian exchange rate returns by considering different frequencies of the series and comparing their one step-ahead forecasts. By analysing the RMSE, U-Theil inequality index, percentage of corrected predicted signals (CPS), and the Pesaran-Timmermann (PT) predictive failure statistic, he found that FLSs and ANNs achieved higher returns based on the forecasts they produced. Similar results were found by @khashei2009 when he analysed the predictive capabilities of FLSs, ANNs, the traditional ARIMA model, and a Fuzzy ARIMA model. 

Although ANNs have been broadly applied in financial forecasting, the process of building them is a complex task and there is no consistent method of design compared to the traditional Box-Jenkins ARIMA model. Unlike ARIMA models, the performance of ANNs is sensitive to many modelling factors such as the number of input nodes included and the size of the training sample chosen [@zhang1998]. Like ANNs there is no systematic approach to designing FLSs and they are only understandable when simple. Although FLSs has the advantage over ARIMA models that they can be applied to data with few observations available, it gives acceptable rather than accurate results and are more suitable for problems which do not require high accuracy.

## Hybrid ARIMA Models
Over time, many researchers began to think of ways in which to harness the advantages of tractable linear models such as the ARIMA model, and of more complex non-linear models. By combining the ARIMA model with other forecasting methods, the advantages of both forecasting methods are leveraged while simultaneously improving their limitations. Some of the hybrid models which have commonly been used in exchange rate forecasting are the Fuzzy ARIMA and ANN-ARIMA model.

The ARIMA model produces very accurate forecasts over short time horizons however it has the limitation of requiring more than 100 observations of historical data to yield accurate results [@tseng2001]. In a world that is constantly changing and with the rapid advancement of technology, access to large amounts of historical data is difficult to obtain. On the contrary, a fuzzy regression model requires little historical data however produces wide prediction intervals if extreme values are present in the data. The Fuzzy ARIMA model combines the ARIMA and fuzzy regression model to exploit the advantages of both models while simultaneously overcoming their limitations. @tseng2001 proposed applying a Fuzzy ARIMA model to forecast the exchange rate of Taiwan dollars to US dollars to demonstrate the model's appropriateness and power. The Fuzzy ARIMA not only produced forecasts that were superior to the ARIMA and fuzzy time series models but also provided an upper and lower bound which can be used by decision makers to determine the best and worst possible situations.

Although ARIMA models are powerful, they require non-stationary data to be differenced and impose prior assumptions onto the distribution of the data [@ince2006]. In contrast, machine learning techniques such as ANNs do not impose any assumptions onto the data generating process however, being data-driven, are sensitive to the number of input nodes used. The ANN-ARIMA model draws on the strengths of both models and overcomes these individual difficulties. @ince2006 created an ANN-ARIMA model by using the ARIMA model to determine the number input nodes required by an ANN for three exchange rates. When the ANN-ARIMA model was compared to the pure ARIMA model, the hybrid model outperformed the ARIMA based on the MSE. The findings of @khashei2011 study agree with these results. They implemented an ARIMA model and used its residuals together with past observations of the data as inputs for the ANN. Their hybrid model had superior in-sample and out-sample forecasts compared to the random walk, linear autoregressive and ANN model.

Although the traditional ARIMA models produce less superior forecasts than its hybrid forms and other complex non-linear techniques, its forecasts are still satisfactory. They are simple models that are easy to implement and have a consistent method of model design and selection. ARIMA models are also more robust and efficient than complex structural models in relation to short-run forecasting. The fact that they have been used as the foundation for more advanced models and have commonly been used as a benchmark for comparison justifies it as a good starting point to compare it to Facebook's forecasting method, Prophet, that was recently released.

## Forecasting with Prophet 
The techniques that have been considered for exchange rate forecasting thus far, require the analyst to have vocational knowledge about time series. Prophet differs to traditional time series models in that it is flexible and can be customised by a large number of non-experts who have little knowledge about time series, however have domain knowledge about the data generating process. Prophet allows for a large number of forecasts to be produced across a variety of problems and consists of a robust evaluation system that allows for a large number of forecasts be evaluated and compared. This is Facebook’s definition of forecasting at scale.

Prophet consists of a decomposable model of the form:

$$ 
	y(t) = g(t) + s(t) + h(t) + e_t
$$
where the components of the model represent the growth, seasonality and holiday respectively, and $e_t$ is white noise.

These components consist of linear and non-linear functions of time. This differs to ARIMA models in which future values of the process are linear functions of previous observations and lagged residuals. Prophet is more like a Generalized Additive Model (GAM) which is a regression model that consists of non-linear and linear regression functions applied to predictor variables [@taylor2017]. Prophet, like the GAM, frames the forecasting problem as a curve fitting exercise, using backfitting to find the regression functions. The GAM is fitted quickly, allowing the analyst to interactively change the model parameters [@taylor2017].

The growth component is modelled in a similar way to population growths which use a logistic growth model [@taylor2017]. Populations typically grow non-linearly (although the growth component could also be linear) up to an upper bound known as the carrying capacity, and remains constant thereafter. The rate at which the population grows changes over time, and this is accounted for by including changepoints in the model where the growth rate can be automatically selected and may be adjusted [@taylor2017]. This allows non-experts with knowledge about events that may affect growth to use the parameter as a knob and adjust it to increase or decrease the number of changepoints [@taylor2017]. It also allows for the analyst to add changepoints which the automatic selection procedure may have missed [@taylor2017]. Furthermore, analysts may also specify the carrying capacity and adjust it based on their knowledge of the total market size [@taylor2017]. 

The decomposable form of the model allows for components to be easily added to it. This allows for multiple seasonality components with different periods to be added to the model. The variance of the parameters of the seasonality component’s distribution can be adjusted by analysts to smooth the model and change how much of historical seasonality is projected to the future [@taylor2017]. 

The name, date, and country of past and future holidays and events may also be inputted by the analyst into a list [@taylor2017]. By specifying the country in which the events take place or the holidays occur, separate lists can be populated for global events/holidays and country-specific events/holidays. The union of the two lists are then used for forecasting. Like seasonality, the variance of the parameters of the holiday component’s distribution, can be adjusted by analysts to smooth the model [@taylor2017].

# Data \label{Data}

# Methodology \label{Methodology}

## The Box-Jenkins Methodology and Automatic Selection
Box and Jenkins proposed a set of guidelines that can be followed when selecting ARIMA models. This systematic procedure to designing ARIMA models has made them highly popular [@spyros1997]. It consists of a four-stage iterative process in which: 1) the process is either transformed or differenced to detrend and stabilise the variance of the data, 2) the autocorrelation and partial autocorrelation plots are used to determine the order of p and q, 3) the parameters of the model are estimated, 4) a diagnostic check is performed to ensure that the residuals are a white noise process. If the residuals are not white noise steps 2-4 are repeated until a satisfactory model is identified. On the contrary, if the diagnostic check shows that the residuals are random then the developed model is the final model used for forecasting.

Due to the large number of forecasts made, it is useful to have an automatic procedure which is able to select the appropriate ARIMA model to fit the data. The auto.arima function in R is able to automatically choose the order of the parameters p, q and d. The order of differencing is determined by using the KPSS test [@ruppert2015]. The KPSS test checks the null hypothesis of stationarity and sets d to zero if the null hypothesis is accepted, otherwise it iteratively increases d by 1 and tests the null hypothesis until it is accepted [@ruppert2015]. Once the order of differencing has been determined, the order of p and q are chosen based on Akaike’s Information Criterion (AIC) or the Bayesian Information Criterion (BIC) [@ruppert2015].

## Semi-Automatic Selection
When a large number of forecasts are produced, manually identifying problematic forecasts becomes a time consuming and difficult task. Prophet provides a semi-automated forecast evaluation system that selects the best model that fits the data. When there are large forecast errors, the forecasts are flagged so that the analyst can explore the cause of the errors, identify and remove potential outliers and either adjust the model or choose a more appropriate model [@taylor2017]. Unlike ARIMA’s fully-automated evaluation system, Prophet provides interactive feedback and keeps the analyst in the loop. 

## Mincer-Zarnowitz Approach to Forecast Evaluation 
The Mincer-Zarnowitz approach to evaluating forecast accuracy is commonly used and can be useful when comparing the forecasts produced by Prophet and the ARIMA model. @mincer1969 proposed an absolute and relative measure to evaluate forecast accuracy. Absolute measures consider the distance between actual and predicted values. To analyse the absolute errors produced by the forecasts, the observed values are regressed against the predicted values. The intercept of the regression equation represents the mean distance between the observed and predicted values while the slope represents the correlation between the residual errors and the predicted values. A zero intercept implies that the forecasts are unbiased and do not overestimate or underestimate the data, while a unity slope implies that the forecasts are efficient and uncorrelated with the residual errors. A joint hypothesis test is performed to check this efficiency and bias of the forecasts, and models which produce the best results are selected.

Absolute forecast measures cannot be used to make comparisons between forecasts with different scales or economic variables. Furthermore, the size of forecasting errors is not as significant as the consequences of forecasting errors and how they impact the decision making process. Hence, rather than using absolute measures of forecast accuracy, @mincer1969 suggested the use of relative measures. Relative accuracy analysis allows for meaningful comparisons of different forecasting methods to be made. It uses an index which considers the ratio of the MSE of the forecast to the MSE of a benchmark forecasting method. A useful benchmark that may be used is an extrapolation of the data history, as it is a cost effective and accessible method, however any method which is relevant for comparison may be used. This ratio is known as the Relative Mean Square Error (RM) for forecast evaluation. The numerator can be viewed as a return which is inversely proportional to the MSE error of the forecasts, while the denominator can be viewed as the cost of forecasting which is inversely proportional to the MSE of the benchmark [@mincer1969]. Hence the ratio is representative of a rate of return index and ranks the performance of forecasts as such. Models that result in forecasts with a RM that is less than one are said to produce superior forecasts to the benchmark model being considered.

# Results \label{Results}

# Discussion and Conclusions \label{Discussion and Conclusions}

\newpage

# References